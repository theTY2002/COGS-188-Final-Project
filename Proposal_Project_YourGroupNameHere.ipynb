{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Jing Yin Yip\n",
    "- Kevin Fisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Mahjong is a strategic, stochastic game that serves as an interesting environment to train a ML agent in. This is due to the sheer amount of hidden information, as well as the complex but well-balanced ruleset. The goal of our project is to train an agent that maximizes its score in a given game, which it can do by winning hands and trying to encourage certain special win conditions. The agent will be trained using temporal difference learning, with a neural network serving as an approximation of the q-table. Performance will be measured by having it play against other agents with abilities expected to be less or equal to its own abilities and seeing what score it can achieve, as well as how often it wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Mahjong is a popular semi-random tile-based game from China, which is also popular in many other Asian countries [1]. Due to its age and popularity, there are many different versions of mahjong, each with slightly different rules. Here we describe Hong Kong mahjong, as it is one of the most popular and simple versions.\n",
    "To start, the 144 tiles (with 34 unique types of tile) are shuffled and placed into the wall, which is where they will be drawn from. Then each player draws 13 tiles, and the starting player draws a tile. They then discard a tile, and this pattern of drawing a tile then discarding continues around the table. The goal is to create pongs (sets of 3 identical tiles) and chows (runs of 3 adjacent numbered tiles) such that your hand of 13 has 4 melds (the general term for pongs and chows) and one pair made from the extra 14th tile you draw and the remaining tile. This can either be from the tiles you draw, or by stealing a discarded tile if it would allow you to make a meld. Once someone wins, they receive points from the other players based on the specific hand they won with, along with some other factors. This then continues for a minimum of 16 hands [1].\n",
    "The state of the art Mahjong AI is Suphx, a reinforcement learning-based agent which uses five smaller models to play Mahjong [2]. This achieves excellent performance compared to other agents and even humans, being better than any other agent and at least comparable to, if not better than, every human. This is an agent for riichi mahjong, also known as Japanese mahjong, which appears to have been chosen as there are online lobbies in which one can test agents against humans (tenhou.net) [3]. However, riichi mahjong is also much more complicated, and is not as commonly played by the Chinese demographic. As such, we will continue to use the rules of Hong Kong-style mahjong, but there are some ideas which are relevant to us in Suphx, as they share a common base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem that we are solving is to find an optimal strategy to maximize the points in a game of mahjong. We will be using Hong Kong mahjong, as this will make it simpler to model, as well as more general, as described above. This is as compared to riichi mahjong, which is much more complicated, and has already been explored to a greater extent.\n",
    "The problem is quantifiable as mahjong has a fixed and defined set of rules, as well as rules for scoring, which is the number we will be maximizing. A given solution to the problem will be measured by the total number of points that a player has at the end of a game (which consists of 4 rounds and a minimum of 16 hands); and a game of mahjong can be replicated since it is deterministic given the wall and the hands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "As there are no existing datasets of Hong Kong-style mahjong that are easily accessible, we will be creating our own dataset by having the model play games against itself. As we will be using temporal difference learning, one observation will be a single state of the game, and we will also record other critical variables, such as the win rate of the model, the accumulated winning hand types, and the amount of points that the model possesses after each episode.\n",
    "We may also have the model play against other simple agents, and possibly even a variation of itself trained separately, as this may allow it to learn to play against more diverse styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our proposed solution is to train a model that aims to maximize the total number of points obtained over the course of an episode, which is defined to be 4 rounds of mahjong. The learning method that we will be using is temporal difference (TD) learning. We chose this over Monte Carlo as our learning method, as an episode potentially contains a large amount of actions (roughly 100), and waiting for each episode to complete before updating our estimates would slow learning considerably. We also chose this over Dynamic Programming as our learning method, as there are a large number of possible states in the state space, and iterating over all of them would be intractable.\n",
    "We will be using TD to train a neural network that will serve as an approximation for the Q-function, as the state space is very large (around 200 GB if fully enumerated). Our inputs to the neural network will consist of the current hand of the agent, the current discarded tiles, the melded tiles of other players, the current wind of the game (which affects payouts), and the current points of all players. There will be 34 outputs of the neural network, each representing the probability of the optimal action being discarding that type of tile. In order to encourage a balance of exploration and exploitation, we will use epsilon-greedy as a strategy to select actions, though we may also explore other action selection strategies.\n",
    "Since the agent only directly receives points at the end of each hand, we would experience very slow learning if this was the only way to estimate value. As such, our reward function will incorporate not only the number of points, but also certain good states, such as having pongs and chows. This should allow the choices of the network to be slightly better guided and speed up convergence.\n",
    "As a proof of concept, an existing model that uses reinforcement learning made by Microsoft has achieved good performance, but it cannot be used as a benchmark model to compare performance, as it plays mahjong with a different set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The primary evaluation metric that we will be using to quantify the performance of the solution model is the average number of points that it ends up with at the end of many episodes. A possible evaluation method to use in lieu of a benchmark model would be to compare its performance against other agents, such as an agent that takes random actions and a traditional agent that is pre-programmed with existing strategies.\n",
    "The scoring system of the game is determined by the number of Fan that a given hand possesses, where different kinds of patterns add more Fan to the total; and different amounts of Fan at the end of a hand determine the score for the winner’s hand. Certain special hands also lead to large amounts of points. For further information, refer to [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a simulated game environment with self-generated data, there are luckily very few ethics concerns. The primary concern, as with any other AI for playing games, is that someone might use this AI to cheat. The benefit of mahjong as a game in this case is that there are really only two modalities of play. The first modality is online, and in this case, not only are there relatively few sites to play Hong Kong mahjong at all, but it would also be difficult to enter the game state and then calculate the appropriate action within the turn timer, as our AI wouldn’t directly hook up to those sites. As for the other modality (in person play), it is very easy to tell whether someone is using an AI to select moves from them, so this is not a concern either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distributing work evenly\n",
    "* Practicing good communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/15  | 4 PM  |  Initial Research  | Write project proposal | \n",
    "| 5/22  | 4 PM  |  Programming the application | Choosing strategies and architecture for training | \n",
    "| 5/29  | 4 PM  | Start training agent  | Training progress   |\n",
    "| 6/5  | 4 PM | Obtain agent performance | Discuss/edit full project |\n",
    "| 6/12  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "1. Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T., Liu, T., & Hon, H. (2020). *Suphx: Mastering Mahjong with Deep Reinforcement Learning.* ArXiv, abs/2003.13590.\n",
    "2. Wikipedia contributors. (2024b, May 11). *Mahjong.* Wikipedia. https://en.wikipedia.org/wiki/Mahjong\n",
    "3. Wikipedia contributors. (2024a, April 14). *Japanese mahjong.* Wikipedia. https://en.wikipedia.org/wiki/Japanese_mahjong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
